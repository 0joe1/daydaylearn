监督学习：

回归：在无穷多个可能的数字中去预测那个正确的数字

分类：在一系列可能的选项中选出正确的类别

无监督算法

不给数据贴标签，让算法自动找到有趣的东西，比如数据特有的结构、模式等

1.聚类算法

自动找出数据的结构特征，将数据划分为不同聚类

2.异常检测

3.数据压缩



一些术语

数据集：用于训练模型的数据集合

x 输入变元

y 输出变元

m 训练样本的总数

(x,y) 单个训练样本

(x^i^,y^i^)  指第 i 个训练样本



输入一个变量x,经过函数 f ，得到 "y-hat"    $\hat{y}$

$\hat{y}$表示一个估计值

单变量线性回归：$f_{w,b}(x)=wx+b$ 其中这个下标可以不写，然后简写为 $f(x) = wx+b$

就是一条直线

有个花里胡哨的名字Univariate



代价函数：


$$
J_{w,b}=\frac1{2m}\sum_{i=1}^{m}(\hat{y}^{(i)} - y^{(i)})^2
$$


梯度下降：

你在一个凹凸不平的山丘堆的某个小山丘上，环顾四周，尝试向走能下山最快的方向走很小的一步。你每走一步都会重复这一步骤。

最终你会到达一个低地，这是一个局部最低点，但你不知道这是否这是全局最低点。

$w=w-\alpha \frac{\partial}{\partial w}J(w,b)$

$b = b-\alpha \frac{\partial}{\partial b}J(w,b)$

这是每迈出一步后w和b的变化情况，其中$\alpha$是你每次迈出的步子。

如果$\alpha$大，就说明你的策略很激进。可能过冲，一下偏左一下偏右，永远都达不到最小值（无法收敛，甚至可能会发散）。

如果$\alpha$小，每次更新的小，就要更新很多次才能达成目标。



越接近山谷，斜率越小，于是每次更新的也小。如果斜率为0,就不更新，这时候就找到了最小点。



上式求偏导：
$$
\frac{\partial}{\partial{w}}J(w,b)=\frac{\partial}{\partial{w}}\frac{1}{2m}\sum_{i=1}^{m}((wx^{(i)}+b)-y^{(i)})^2=\frac{1}{m}\sum_{i=1}^{m}(wx^{(i)}+b){x^{(i)}}
$$

$$
\frac{\partial}{\partial{b}}J(w,b)=\frac{\partial}{\partial{b}}\frac{1}{2m}\sum_{i=1}^{m}((wx^{(i)}+b)-y^{(i)})^2=\frac{1}{m}\sum_{i=1}^{m}(wx^{(i)}+b)
$$





多元线性回归
$$
\vec{w}=[w_1,w_2,w_3...w_n]
$$

$$
\vec{x}=[x_1,x_2,x_3...x_n]^T
$$

$$
f_{\vec{m},b}(x)=\vec{w}\cdot\vec{x}+b=w_1x_1+w_2x_2+w_3x_3+...+w_nx_n+b
$$

